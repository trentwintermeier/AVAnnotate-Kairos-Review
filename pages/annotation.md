---
layout: item
title: Annotation
manifest_name: annotation
permalink: annotation
external_manifest_url: 

---
<!-- Add an essay or interpretive material below this line,
using HTML or markdown.  Do not modify this file above this line -->
<head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VE0VSZDWME"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VE0VSZDWME');
</script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VE0VSZDWME');
</script>

<div id="myDIV" style="display: none; text-align: justify">
  After finding usable audio and creating a manifest, annotating such audio is the next step. I’ve found this step to be really simple and thought provoking as it offers the first part of a unique practice of listening which the software offers. The only requirements for annotating with AVAnnotate is access to and minimal knowledge of Excel spreadsheets or Google Sheets. Since AVAnnotate doesn’t offer a way to annotate within the software, annotation happens with other tools. This is similar to how GitHub repositories, Jekyll, and IIIF are integrated—Clement and Brumfield Labs referred to this as “glue code—code that sticks a bunch of things together to make something useful” (2022). Thus, instead of building in annotation as part of the software, AVAnnotate “out sources,” in a sense, this functionality. It’s pretty simple, and it’s much better than learning or downloading another spreadsheet software, as I already own and am familiar with both Excel and Google Sheets.
<br><br>
The spreadsheet is organized in five columns, with only three being mandatory. The first vertical column is the first time stamp, the first time in the audio where the annotation begins. That goes in column A in the format of hh:mm:ss. And the next column, column B, includes the time where that annotation stops, in the same hh:mm:ss format. If, for example, I wanted to annotate part of an audio recording of someone who speaks from minute 1 to minute 2, I would put in column A “00:01:00” and column B “00:02:00.” Or, if I wanted to make a “point” annotation, I could put just the beginning of that speaker at “00:01:00” in column A and “00:01:00” in column B. This practice of making timestamps is really helpful for certain audio as it creates a different process and purpose to listening. In just the time stamping, I’ve found that I’m more focused on what and where a sound is, and focusing less on what that sound means. The time stamping required by AVAnnotate asks users to locate, but not define, certain moments in audio recordings. Such an act creates a conceptual break that is not often experienced in common practices of listening.
<br><br>
The next column, C, is designed for the annotation of the prior time stamps. This section is where I believe AVAnnotates offers the most valuable flexibility and engagement with access to information. This annotation can be almost anything. Column C can be a transcription, observation, image (using Markdown), symbols, or general thoughts—really, users can write anything in this section. I understand this to be extremely beneficial and important for how access is communicated in digital spaces. Indeed, transcription is helpful for quickly searching an audio file, assisting users with auditory disabilities, creating metadata, and more. However, having the opportunity to include ideas, thoughts, visuals, and links according to certain portions of audio is a different practice of making meaning for sustainable access to information. In my personal experience of annotation, I’ve found two things: First, AVAnnotate becomes a unique tool for knowledge dissemination when the possibilities of digital annotation are exercised to their full potential. Being able to communicate with images and external data creates new ways to experience information according to goals of access. In other words, communicating for sustainable access through digital annotation develops new and different ways of knowing audio. And second, to this same point, the act of listening for the use of sound is a unique experience offered by AVAnnotate. The software teaches users to engage with sound differently by emphasizing what sound can do for access, and how we, as annotators, participate in what this sound means.
<br><br>
Further, the last two columns, D and E, are for “Layers” and “Index,” respectively. The former is a form of categorization of the annotation is column C. The “Index” column is, simply, for an index which can be added to the project. I’ve found these both to be helpful additions to the software, but only if the user knows what their annotations are doing/ what they are for. In other words, having all of these columns requires up-front conceptual work by the user in that they need to decide the purpose of their project’s annotations. Even if the sole purpose of an AVAnnotate exhibit is to promote sustainable access to an audio artifact, each annotation requires decisions about what is highlighted and what isn’t. Is access to information about context important to this audio? A transcript of one or multiple speakers? The background noises or quality of the audio? Having layers and an index requires users to acknowledge how they are making meaning with the software—which is beneficial but difficult work.
  <br><br>
</div>

<link rel="stylesheet" type="text/css" href="style.css">
</head>
